{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>a31606.txt</td>\n",
       "      <td>ive never tried it. But i read experimention, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>a69565.txt</td>\n",
       "      <td>i want want to be right with god and also i wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>a61450.txt</td>\n",
       "      <td>Sugar cannot be healthy.  Sugar cane can be he...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>a69614.txt</td>\n",
       "      <td>huh???\\neating them... did u do it???\\nplzzzzz...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>a69535.txt</td>\n",
       "      <td>To have the highest point count on yahoo answe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>a7367.txt</td>\n",
       "      <td>great answer, lets vote</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>a24679.txt</td>\n",
       "      <td>It appears that the question period has expire...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>a24387.txt</td>\n",
       "      <td>Its easy. Just eat a healthy, plant-based diet...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>ans822.txt</td>\n",
       "      <td>Lamictal (lamotrigine) can cause side-effects ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>ans1740.txt</td>\n",
       "      <td>While your headaches and feeling faint may be ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>a7352.txt</td>\n",
       "      <td>Sin entered the world and as punishment, God s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>ans848.txt</td>\n",
       "      <td>Maybe your symptoms have to do with a varicose...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>a24343.txt</td>\n",
       "      <td>I am not sure, but maybe it is a form of an al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>a54238.txt</td>\n",
       "      <td>Well, depends on what activities you partake i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>a69618.txt</td>\n",
       "      <td>NOPE, not even if you swallow gallons of sperm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>a24601.txt</td>\n",
       "      <td>try taking cold pills drink lots of fluid and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>a61363.txt</td>\n",
       "      <td>chill out listen to cool music come on here an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>a24775.txt</td>\n",
       "      <td>I do not suggest you use any Home Remedies for...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             file                                               text  class\n",
       "682    a31606.txt  ive never tried it. But i read experimention, ...      0\n",
       "1432   a69565.txt  i want want to be right with god and also i wo...      0\n",
       "1150   a61450.txt  Sugar cannot be healthy.  Sugar cane can be he...      0\n",
       "1479   a69614.txt  huh???\\neating them... did u do it???\\nplzzzzz...      0\n",
       "1402   a69535.txt  To have the highest point count on yahoo answe...      0\n",
       "1579    a7367.txt                           great answer, lets vote       0\n",
       "447    a24679.txt  It appears that the question period has expire...      0\n",
       "155    a24387.txt  Its easy. Just eat a healthy, plant-based diet...      0\n",
       "1678   ans822.txt  Lamictal (lamotrigine) can cause side-effects ...      1\n",
       "824   ans1740.txt  While your headaches and feeling faint may be ...      1\n",
       "1564    a7352.txt  Sin entered the world and as punishment, God s...      0\n",
       "1706   ans848.txt  Maybe your symptoms have to do with a varicose...      1\n",
       "111    a24343.txt  I am not sure, but maybe it is a form of an al...      0\n",
       "832    a54238.txt  Well, depends on what activities you partake i...      0\n",
       "1483   a69618.txt  NOPE, not even if you swallow gallons of sperm...      0\n",
       "369    a24601.txt  try taking cold pills drink lots of fluid and ...      0\n",
       "1063   a61363.txt  chill out listen to cool music come on here an...      0\n",
       "543    a24775.txt  I do not suggest you use any Home Remedies for...      0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data into a pandas dataframe\n",
    "import os\n",
    "def data2df (path, label):\n",
    "    file, text = [], []\n",
    "    for f in os.listdir(path):\n",
    "        file.append(f)\n",
    "        fhr = open(path+f, 'r',encoding='utf-8', errors='ignore') \n",
    "        t = fhr.read()\n",
    "        text.append(t)\n",
    "        fhr.close()\n",
    "    return(pd.DataFrame({'file': file, 'text': text, 'class':label}))\n",
    "\n",
    "dfnonpro = data2df('C:\\\\Users\\\\LENOVO\\\\Documents\\\\Varun\\\\ASU\\\\Wolf Pack\\\\CIS 509\\\\Week 6\\\\Assignment 5\\\\HealthProNonPro\\\\HealthProNonPro\\\\NonPro\\\\', 0) # Nonpro\n",
    "dfpro    = data2df('C:\\\\Users\\\\LENOVO\\\\Documents\\\\Varun\\\\ASU\\\\Wolf Pack\\\\CIS 509\\\\Week 6\\\\Assignment 5\\\\HealthProNonPro\\\\HealthProNonPro\\\\Pro\\\\', 1) # Pro\n",
    "\n",
    "df = pd.concat([dfpro, dfnonpro], axis=0)\n",
    "df.sample(frac=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the data\n",
    "X, y = df['text'], df['class']\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=1) #30% randomly slected instances in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a custom preprocessor\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def preprocess(text):\n",
    "    # replace one or more white-space characters with a space\n",
    "    regex = re.compile(r\"\\s+\")                               \n",
    "    text = regex.sub(' ', text)    \n",
    "    # lower case\n",
    "    text = text.lower()          \n",
    "    # remove digits and punctuation\n",
    "    regex = re.compile(r\"[%s%s]\" % (string.punctuation, string.digits))\n",
    "    text = regex.sub(' ', text)           \n",
    "    # remove stop words\n",
    "    sw = stopwords.words('english')\n",
    "    text = text.split()                                              \n",
    "    text = ' '.join([w for w in text if w not in sw]) \n",
    "    # remove short words\n",
    "    ' '.join([w for w in text.split() if len(w) >= 2])\n",
    "    # lemmatize as stemming leads to loss of vital information at times \n",
    "    text = ' '.join([(WordNetLemmatizer()).lemmatize(w) for w in text.split()]) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the preprocessing->model pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "clf = Pipeline(steps=[\n",
    "    ('pp', TfidfVectorizer(\n",
    "        preprocessor=preprocess,\n",
    "        lowercase=True, stop_words='english', \n",
    "        use_idf=True, smooth_idf=True,\n",
    "        min_df=1, max_df=1.0, max_features=None, \n",
    "        ngram_range=(1, 1))),\n",
    "    ('mdl',     MultinomialNB())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'pp__norm': ['l1','l2',None],\n",
    "    'mdl__alpha':[0.01, 0.1, 0.2, 0.5, 1]\n",
    "}\n",
    "gscv = GridSearchCV(clf, param_grid, iid=False, cv=5, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('pp', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function preproc...se_idf=True, vocabulary=None)), ('mdl', MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# search for best parameters/estimator\n",
    "gscv.fit(Xtrain, ytrain)\n",
    "\n",
    "print(gscv.best_estimator_, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9463148316651502\n",
      "[[476  54]\n",
      " [  5 564]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94       530\n",
      "           1       0.91      0.99      0.95       569\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1099\n",
      "   macro avg       0.95      0.94      0.95      1099\n",
      "weighted avg       0.95      0.95      0.95      1099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate best_estimator_ on test data\n",
    "ypred = gscv.best_estimator_.predict(Xtest)\n",
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(ytest, ypred))\n",
    "print (metrics.confusion_matrix(ytest, ypred))\n",
    "print (metrics.classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
